{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "GMQiZwjn3iu7",
        "1UUpS68QDMuG",
        "P1XJ9OREExlT",
        "EyNgTHvd2WFk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush2444/PlayStore_Data-Analysis/blob/main/ML_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Seoul Bike Sharing Demand Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike sharing systems provide a convenient method for renting bicycles through a network of kiosks located throughout a city, where individuals can obtain membership, rent a bike, and automatically return it to any other kiosk location as per their requirement.\n",
        "\n",
        "\n",
        "With over 500 bike-sharing programs operating globally, the trend of bike and scooter ride-sharing has gained significant momentum, particularly in metropolitan areas such as San Francisco, New York, Chicago, and Los Angeles. However, predicting bike demand on a specific day remains a crucial challenge for these businesses.\n",
        "\n",
        "The bike sharing system market is picking up speed and gaining momentum globally. In 2019, its market share was valued at an impressive 3.39 billion, and industry experts predict that it will surge to a whopping $6.98 billion by 2027, with a projected compound annual growth rate of around 14% from 2020 to 2027.\n",
        "\n",
        "\n",
        "Our objective is to develop a predictive model that can estimate the approximate number of bikes rented based on the available dataset, considering that bike sharing systems typically rent bikes on an hourly, daily, or monthly basis, with static pricing inclusive of these time periods. The system's affordability and user-friendly renting process have made it a popular choice for commuters of all kinds.\n",
        "\n",
        "Our project aims to utilise historical bike usage patterns and weather data to forecast the demand for bike rentals. The dataset we are working with comprises hourly rental data for a span of two years. Specifically, the training set includes data from the first 19 days of each month, while the test set encompasses the period from the 20th to the end of each month.\n",
        "\n",
        "Our initial step is to conduct an Exploratory Data Analysis on the dataset. We examine the presence of any missing data values, though none were found, and identify and address any outliers in the dataset. Moreover, we carry out correlation analysis to identify the relevant and significant feature set, which we later modify through feature engineering. This involves adjusting a few existing columns and dropping any irrelevant ones from the dataset.\n",
        "\n",
        "\n",
        "\n",
        "then further on Through the process of feature engineering and data preprocessing, we aimed to identify and isolate impactful features for our analysis. One of the initial steps in this process was to address multicollinearity within the independent variables. We accomplished this through the use of various inflation factor (VIF) measures. then we utilized the interquartile range (IQR) technique to detect and treat outliers in our data. We then capped all outliers of continuous features between the 25th and 75th percentile.\n",
        "\n",
        "We also noted that certain features were categorical in nature, and as such, were unsuitable for input into a machine learning model in their current form. To address this, we encoded these features into numerical values using the One-Hot Encoding technique, as they were unordered in nature.\n",
        "\n",
        "Following that, we have divided our dataset  into training and testing sets. The model is then trained using the selection of a machine learning algorithm and the training set of data. In order to determine how effectively the model can predict rented bike count, we lastly assessed its performance on the testing data\n",
        "\n",
        "Our exploration of machine learning models for the Bike Sharing Demand dataset took us on a journey through various popular approaches. We started with the tried-and-true Linear Regressor, as well as Regularization Models like Ridge and Lasso, and even ventured into the realm of elasticnet. But we didn't stop there; we also delved into more sophisticated ensemble models like Random Forest, decision trees, and Gradient Boost.\n",
        "\n",
        "Constructing a machine learning model for the Bike Sharing Demand Prediction dataset required a combination of data preprocessing, utilization of various machine learning techniques, and adept model evaluation skills. Despite the challenges, we were able to develop a high-performing model that can effectively predict  the demand of rented bikes. Among the multiple models we trained, XGboost outperformed the others and gave the highest r2 score.  **Finally**, we have developed a model that can successfully predict the demand for rented bikes."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mu8Bp7myyJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error  \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "URL='https://drive.google.com/file/d/1dZ7p614gC_iwxHwcj-1N0Lc155AGMTJS/view?usp=share_link'\n",
        "df= pd.read_csv('https://drive.google.com/uc?id='+URL.split('/')[-2] , encoding= 'unicode_escape')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "ev2jjG71B06E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "FbbCI-1d9sxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes.astype(str).value_counts())"
      ],
      "metadata": {
        "id": "DViMpoL7Gpdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate = df.duplicated().sum()\n",
        "print('Total number of duplicate values are : ', duplicate)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* The dataset does not contain any duplicate rows or missing values.\n",
        "\n",
        "*  Several feature names are quite long; perhaps they should be renamed\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KK2GD8zdJanB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"column_name\", size=14, weight=\"bold\")\n",
        "plt.title(\"missing values in column\",fontweight=\"bold\",size=17)\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "9CXwut1HHfil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'SeoulBikeData' dataset contains 8760 rows and 14 columns, and is free of any null or duplicate values. Additionally, there are four categorical features within this dataset, namely Date, Season, Holiday, and Functioning Day."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset includes date information, the number of bikes rented every hour, and weather data (temperature, humidity, windspeed, visibility, dewpoint, solar radiation, snowfall, and rainfall).**\n",
        "\n",
        "\n",
        "\n",
        "* **Date** : year-month-day\n",
        "*  **Rented Bike count** - Count of bikes rented at each hour\n",
        "* **Hour** - Hour of he day\n",
        "* **Temperature**-Temperature in Celsius\n",
        "*  **Humidity** - Humidityin the air in %, type: int\n",
        "* **Wind speed (m/s)** - Speed of the wind in m/s\n",
        "*  **Visibility** - 10m\n",
        "* **Dew point temperature(°C)** - The temperature at which the water starts to condense out of the air,\n",
        "* **Solar radiation** - MJ/m2-Electromagnetic radiation emitted by the Sun\n",
        "*   **Rainfall** - mm\n",
        "*   **Holiday** - If the day is holiday or not\n",
        "*   **Seasons** - Winter, Spring, Summer, Autumn\n",
        "*  **Functional Day** - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
        "* **Snowfall** -Amount of snowfall in cm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dug-bUTNxcqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Renaming the features"
      ],
      "metadata": {
        "id": "gwtWTns7TMIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df =df.rename(columns= {'Temperature(°C)':'temperature','Rented Bike Count': 'rented_bike_count', \n",
        "                        'Hour':'hour', 'Humidity(%)':'humidity',  'Dew point temperature(°C)':'dew_point_temp',\n",
        "                        'Wind speed (m/s)': 'wind_speed','Visibility (10m)': 'visibility', 'Solar Radiation (MJ/m2)': 'solar_radiation',\n",
        "                        'Seasons':'seasons', 'Functioning Day':'functioning_day', 'Holiday':'holiday', 'Snowfall (cm)':'snowfall','Rainfall(mm)': 'rainfall'})"
      ],
      "metadata": {
        "id": "-u88fu-vTfz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()  "
      ],
      "metadata": {
        "id": "yLOt4sdoG95j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"the unique values of seasons are :\", df['seasons'].unique())\n",
        "print(\"the unique values of holidays are :\", df['holiday'].unique())\n",
        "print(\"the unique values of functioning_day  are :\", df['functioning_day'].unique())\n",
        "print(\"the unique values of date are :\", df['Date'].unique())\n",
        "\n",
        "print(\"the unique values of hour are :\", df['hour'].unique())\n",
        "print(\"the unique values of humidity are :\", df['humidity'].unique())\n",
        "print(\"the unique values of temperature are :\", df['temperature'])\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing datetime \n",
        "import datetime as dt\n",
        "df['Date'] =df['Date'].apply(lambda x:  dt.datetime.strptime(x,'%d/%m/%Y'))\n",
        "     "
      ],
      "metadata": {
        "id": "LIjLWs1uK_l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the year and month from the date feature\n",
        "df['year'] = df['Date'].dt.year\n",
        "df['month'] =df['Date'].dt.month\n",
        "df['weekday'] = df['Date'].dt.weekday\n"
      ],
      "metadata": {
        "id": "9B9Om2XmLHrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert into category \n",
        "cat =['month','weekday', 'hour', 'year']\n",
        "for i in cat:\n",
        "  df[i]=df[i].astype('category')"
      ],
      "metadata": {
        "id": "jpBKO6BHLM-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## dropping ther date columns \n",
        "df = df.drop(columns = ['Date'], axis = 1)"
      ],
      "metadata": {
        "id": "TYJFWkyxLQNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Yg2sVOuMLW_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data for the year\n",
        "df['year'].value_counts()"
      ],
      "metadata": {
        "id": "y7YKOr5jLbCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# year month count from the data set\n",
        "df.groupby(['year','month']).agg({'rented_bike_count':['sum']}).reset_index()"
      ],
      "metadata": {
        "id": "Clgl07ZxLd3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EDA (Exploratory Data Analysis)** ##\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##**Univariate Analysis**##"
      ],
      "metadata": {
        "id": "T-BOwgqEnuaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "ax = sns.distplot(df['rented_bike_count'], hist=True , color = 'blue')\n",
        "plt.xlabel('Rented_Bike_count')\n",
        "plt.ylabel('Density')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have picked this chart to check the distribution of the dependent variable "
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the distribution of the dependent variable must be normal for linear regression to work, the dependent variable appears to be moderately right skewed in the distribution plot shown above. As a result, we should perform some operations to make the distribution of the dependent variable normal."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "sns.boxplot(x=df['rented_bike_count'])"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Outliers can be found in the rented bike count data, as seen by the boxplot above."
      ],
      "metadata": {
        "id": "tFcLYQnXb4nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the skewed data by using the square root transformation.\n",
        "sns.boxplot(x = np.sqrt(df['rented_bike_count']))"
      ],
      "metadata": {
        "id": "FuDs9qwOqjah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have picked this boxplot to check the outlier in dependent variable"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the removal of outliers from the square root transformation."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# visualizing the distribution of dependent variable after sqrt transformation\n",
        "plt.figure(figsize=(16,10))\n",
        "ax=sns.distplot(np.sqrt(df['rented_bike_count']), color=\"blue\")\n",
        "ax.axvline(np.sqrt(df['rented_bike_count']).mean(), linestyle='dashed', color='red', linewidth=2)\n",
        "plt.xlabel('Rented_Bike Count')\n",
        "plt.ylabel('Density')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose this graph to determine whether or not we now have a normal distribution."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain an almost normal distribution after applying the square root to the skewed Rented Bike Count. As a result, we may perform the square root transformation during modelling."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Exploring Numerical features"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Numeric features"
      ],
      "metadata": {
        "id": "HcXugmeBacoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features which are continuous variable\n",
        "numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O' and feature not in [ 'month', 'year','date' , 'weekday'] ]\n",
        "numerical_features "
      ],
      "metadata": {
        "id": "0hImStE9wGtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#With displot, examine the distribution of each numerical feature.\n",
        "for num in numerical_features :\n",
        "  plt.figure(figsize=(12,5))\n",
        "  sns.distplot(x=df[num] ,color=\"purple\" )\n",
        "  plt.xlabel(num)\n",
        "  plt.show() \n",
        "\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A distplot chart helps us to examine the density of data and calculate the mean of a data column. When a distribution is skewed, the mean and median become skewed as well. In fact, skewed features can tug on both the mean and median, causing them to lean towards the side of the skewness."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above displot that Normally distributed attributes: temperature , hour , humidity. Positively skewed attributes: wind, rented bike count , solar_radiation, snowfall, rainfall. Negatively skewed attributes: visibility. , "
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing positively and negatively distributed data using a distplot chart can provide valuable insights for businesses. identifying patterns in positively and negatively distributed data can help businesses identify areas for improvement and prioritize resources accordingly. Ultimately, using data visualization tools like distplot can help businesses make data-driven decisions and improve overall performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bivariate analysis.\n",
        "\n",
        "Let's strive to determine how numerical variables relate to our dependant variable."
      ],
      "metadata": {
        "id": "7Hv2lWfEHoX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Regression plot , Numeric feature Vs rented bike count"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_variable = [ 'temperature', 'wind_speed','dew_point_temp',  'humidity', 'visibility',  'rainfall', 'snowfall','solar_radiation' ]"
      ],
      "metadata": {
        "id": "eL4vIYfNMVd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rented_bike_count = ['rented_bike_count']"
      ],
      "metadata": {
        "id": "Cl9jnyNWXHIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in continuous_variable:\n",
        "  plt.figure(figsize=(13,6))\n",
        "  sns.regplot(x=df[i], y=df['rented_bike_count'], scatter_kws={\"color\": \"cyan\"}, line_kws={\"color\": \"red\"})\n",
        "  plt.title(f'rented_bike_count vs {i}')\n",
        "  plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "p2fPiPuPJvGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have taken this chart up here to identify the relationship between the dependant and independent variables and select the best fit line since a regression plot is an effective visualisation tool for studying the relationship between two variables."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Temperature**: A positive correlation exists between the two. Between 20 and 30 °C, the number of rented bikes is at its peak. It follows that the effects of temperature exist.\n",
        "\n",
        "**Visibility**: We don't know much about how visibility influences our outcomes, but we do know that it is related with the number of rented bike count.\n",
        "\n",
        "**Dew point**: To get a relative humidity, air must be cooled to the dew point, which is the temperature it must reach (while maintaining a constant pressure). With the data, it has a positive correlation.\n",
        "\n",
        "**Wind speed**: The amount of wind has little impact on our data.\n",
        "\n",
        "**Humidity** : The level of air moisture is known as humidity. Individuals therefore like borrowing bikes when the humidity is lower.\n",
        "\n",
        "**SnowFall and Rainfall** : Individuals avoid borrowing bikes in locations with snowfall or rain when those conditions exist.\n",
        "\n",
        "**'Solar_Radiation'** : are positively related to the dependent variable.\n",
        "\n",
        "\n",
        "\n",
        "* Hour, Temperature, Wind Speed, Visibility, and Solar Radiation are all  positively correlated with the dependant variable. This implies that the number of rented bikes rises as these features do, while the columns \"Rainfall,\" \"Snowfall,\" and \"Humidity\" are those features that have a negative relationship with the dependent variable, suggesting that the number of rented bikes falls as these features rise."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Considering the correlation between the dependent variable and categorical independent variables, Attempting to extract some crucial information from the category feature**"
      ],
      "metadata": {
        "id": "wvQyr8okfNYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6  - Year Vs Rented bike count"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=df.year,y=df['rented_bike_count'], palette=(\"pastel\"))\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel(\"Rented_Bike_Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have taken this bar plot to check the data contains from which year "
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our collection primarily includes data from the year 2018 and only a little amount from the year 2017."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Season Vs Rented bike count\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(15,9))\n",
        "sns.barplot(x=df.seasons,y=df['rented_bike_count'],palette=(\"RdYlGn_r\"))\n",
        "plt.xlabel('Seasons')\n",
        "plt.ylabel(\"Rented_Bike_Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to analyse the data that is divided into four seasons, we have chosen a bar chart to show the number of rented bikes that have been recorded."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see clearly from the above bar graph,  We have found that average of the rented bike counts is higher during the summer and lowest during the winter.\n",
        " it is evident that people enjoy riding bicycles in the summer and the autumn."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By analyzing the graph, we have unlocked valuable insights that can steer the business towards a positive impact. The data suggests that the demand for bikes follows a seasonal pattern, with a peak in the Summer months, followed by a  Autumn, a refreshing Spring, and a chilly Winter. Based on this knowledge, we can strategically plan and maximize our profits during the summer , Autum and spring seasons, while also devising innovative ways to tackle the off-season. "
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDL1XVWHIWIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Month Vs Rented bike count"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(15,9))\n",
        "sns.barplot(x=df.month,y=df['rented_bike_count'],palette=(\"Oranges\"))\n",
        "plt.xlabel('Months')\n",
        "plt.ylabel(\"Rented_Bike_Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There We have created a bar plot to analyse the quantity of bikes that are rented. We can clearly see from the data that this bar plot enables us to determine which month has a higher demand for rented bikes."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the months of December, January, and February—the winter seasons—have lower demand for rented bikes than those months, as well as May, June, and July—the summer seasons—which have the highest demand for bikes."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through careful analysis of the graph, we have uncovered valuable insights that can help us generate a positive impact on our business. It is evident that the demand for bikes fluctuates on a monthly basis, indicating the importance of seasonality in our business planning. Upon closer examination, we can observe that the demand for bikes is at its peak during the scorching months of May, June, and July, while remaining moderate during the months of March, April, August, September, and November. Conversely, the demand for bikes is at its lowest during the colder months of January, February, and December. Armed with this knowledge, we can make informed decisions and tailor our strategies to optimize profits during peak months while developing innovative approaches to tackle the low-demand months. So let's pedal towards prosperity!"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Week_Day Vs rented bike count"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "qZKrJIEfwQIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.barplot(x=df.weekday,y=df['rented_bike_count'],palette=(\"brg_r\") )\n",
        "plt.xlabel('days')\n",
        "plt.ylabel(\"Rented_Bike_Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is used when you want to show a distribution of data points or perform a comparison of metric values across different subgroups of your data\n",
        "\n",
        "There, we used a bar plot to determine which day of the week has the highest demand for rental bikes."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above graph the , all days, rented bike count is consistant and equal"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10- holiday, No holiday"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the frequency count of 'holiday' column\n",
        "holiday_counts = df['holiday'].value_counts()\n",
        "holiday_counts"
      ],
      "metadata": {
        "id": "KyujMMAK-ov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Generate a pie chart to visualize the distribution\n",
        "plt.figure(figsize=(10,8))\n",
        "ax = plt.subplot(111)\n",
        "plt.pie(holiday_counts, \n",
        "        autopct=\"%1.1f%%\",\n",
        "        startangle=90,\n",
        "        shadow=True,\n",
        "        labels=['No Holiday(%)','Holiday(%)'],\n",
        "        colors=['blue','red'],\n",
        "        explode=[0,0])\n",
        "plt.title('Distribution of Holiday ')\n",
        "ax.legend(bbox_to_anchor=(1, 0, 0.5, 1))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jgeCFVzq7uzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart helps organize and show data as a percentage of a whole. When comparing different percentages, pie charts are widely utilised. I thus utilised a pie chart, which enabled me to compare the variable's percentages."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By analyzing the pie chart, I am able to see the valuable insights about the distribution of the 'holiday' column in my dataset. The chart revealed that the majority of the ratings - a whopping 95.1% or 8,328 records - were on non-holiday days. In contrast, the number of ratings received during holidays was relatively low, accounting for only 4.9% or 432 records of the total rented bike count data available in the dataset. These findings highlight the importance of considering external factors, such as holidays, when analyzing data, as they can have a significant impact on the trends and patterns observed in the dataset."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can analyze that the demand for rented bikes is significantly higher on non-holiday days than on holidays. This crucial insight can have a profound impact on business decisions, as it implies that the rental bike business is poised for significant growth during non-holiday periods. On the other hand, the data reveals that the demand for rented bikes during holidays is negligible, indicating a negative growth trend for the business during these times."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Average rented bikes per hour"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.barplot(x=df.hour,y=df['rented_bike_count'])\n",
        "plt.xlabel('hour')\n",
        "plt.ylabel(\"Rented_Bike_Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have picked this bar chart to analyze the demand of rented bike per hour "
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "People favour rented bikes during rush hour, as evidenced by the high surge in hired bikes from 8:00 am to 9:00 pm. We may state that during business opening and closing times there is a significantly high demand because it is apparent that demand increases most at 8 a.m. and 6:00 p.m."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " These findings provide valuable insights into the usage patterns of rented bikes, suggesting that businesses should focus their marketing efforts on promoting the convenience and affordability of rented bikes during peak hours to capitalize on this trend. By doing so, businesses can enhance their profitability and better cater to the needs of their customers."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Functioning Day and Non Functional day"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "mDdok2FxHjXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Generate a pie chart to visualize the distribution\n",
        "plt.figure(figsize=(10,8))\n",
        "ax = plt.subplot(111)\n",
        "plt.pie(df['functioning_day'].value_counts(), \n",
        "        autopct=\"%1.1f%%\",\n",
        "        startangle=90,\n",
        "        shadow=True,\n",
        "        labels=['Functional day(%)','Non functional day(%)'],\n",
        "        colors=['blue','red'],\n",
        "        explode=[0,0])\n",
        "plt.title('(%) of functioning day and Non functional day ')\n",
        "ax.legend(bbox_to_anchor=(1, 0, 0.5, 1))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xmcDwE9LHcT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart helps organize and show data as a percentage of a whole. When comparing different percentages, pie charts are widely utilised. I thus utilised a pie chart, which enabled me to compare the variable's percentages."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the chart, it can be observed that 96.6% of the dataset consists of non-functional days, while the remaining 3.4% represents weekends. This indicates that there is a significantly higher demand for bikes on functional days compared to weekends, where the demand is relatively low."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By analyzing the chart, we can conclude that functional days are the prime time for bike rentals, with a remarkable surge in demand. However, the weekends exhibit a different story as there is minimal demand for bike rentals. This valuable insight is crucial from a business perspective, as it presents an opportunity to capitalize on the growth potential during functional days and to strategize ways to tackle the decline in demand during non-functional days. In essence, functional days act as a catalyst for business growth, while weekends can pose a challenge to profitability."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chart - 13 - Snowfall Vs rented bike count\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iYxhLkCj42kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.barplot(x=df.snowfall,y=df['rented_bike_count'])\n",
        "plt.xlabel('snowfall')\n",
        "plt.ylabel(\"Rented_Bike_Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gawmGBRBddDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "FWQiOBFvdk-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to examine the provided data, we have chosen a bar chart to show the relationship between the number of rented bikes and the amount of snowfall."
      ],
      "metadata": {
        "id": "kXWZTytsdtKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "AsUqtXvTdzkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe a decline in demand for rented bikes when snow falls."
      ],
      "metadata": {
        "id": "uPlYqIcwd0ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "\n",
        "\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "1aL-mFARd-4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of the graph suggests that snowfall has a significant impact on the demand for rented bikes, resulting in a considerable decrease. This decrease in demand, if not accounted for, can have a negative impact on the business. Therefore, it is crucial for businesses to be aware of the weather conditions and adjust their strategies accordingly to optimize growth potential and ensure the smooth running of operations."
      ],
      "metadata": {
        "id": "38BHuKRoieio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In order to represent the number of rented bikes across several categorical parameters with regard to hour, we create point plots."
      ],
      "metadata": {
        "id": "ckLwO6KV-2PL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 Average Bike Rented_per hour"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "fig,ax=plt.subplots(figsize=(18,8))\n",
        "Average_Rent_hours =df.groupby('hour')['rented_bike_count'].mean()\n",
        "ax=Average_Rent_hours.plot(legend=True, marker='o', title=\"Average_Bikes_Rented_Per_Hour\")\n",
        "ax.set_xticks(range(len(Average_Rent_hours)))\n",
        "ax.set_xticklabels(Average_Rent_hours.index.tolist(), rotation=90)\n",
        "\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "azctHA2t51mF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have picked this bar chart to analyze the demand of rented bike per hour "
      ],
      "metadata": {
        "id": "ZmXrLyJx52Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "t8fAn8Cl556g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By observing the graph, it is clear that there is a sharp increase in the use of rented bikes between the hours of 8:00 a.m. and 9:00 p.m., suggesting that individuals prefer to use rented bikes during peak hours, perhaps for their commute to work. In addition, it can be seen that the demand for rental bikes is greater on non-holiday days than it is on days with holidays."
      ],
      "metadata": {
        "id": "a_z9bzRg56f0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "gNwz2zGc598l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This valuable insight is critical from a business perspective, as it highlights the growth potential during non-holiday days, while also underlining the challenge of low demand during holidays.\n",
        "\n",
        "The peak hours during non-holiday days present a significant opportunity for business growth, whereas holidays can result in a sharp decline in profitability."
      ],
      "metadata": {
        "id": "fslwR04qog8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chart - 15 Average rented bike per day"
      ],
      "metadata": {
        "id": "MbTk-vA34nyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15,9))\n",
        "sns.pointplot(data=df,x='hour',y='rented_bike_count', palette=(\"Reds_r\"),hue='weekday' , dodge=True, ci= None)\n",
        "ax.set_xlabel('Hour',fontsize=15)\n",
        "ax.set_ylabel('Rented_Bikes_Count',fontsize=15)\n",
        "ax.set_title('Average Rented bikes per day' , fontsize=15)"
      ],
      "metadata": {
        "id": "_nfmIcQ6DNSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "DwlzU1my5p5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have taken this plot to analyze which day has the demand in a week"
      ],
      "metadata": {
        "id": "6VCvlAtN5q-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "chQdsblZ5t-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By analyzing the Graph, it can be concluded that the average number of rented bikes remains relatively stable from Monday to Saturday. However, there is a noticeable dip in bike rentals on Sundays, and on average, the number of rented bikes is significantly lower on weekends than on weekdays"
      ],
      "metadata": {
        "id": "tW-kosKX5uzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "nwR47AHb5x_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This valuable insight presents a unique opportunity for businesses to capitalize on the stable demand for rented bikes during weekdays, while also strategizing ways to overcome the challenge of low demand on weekends."
      ],
      "metadata": {
        "id": "lcRhMxBHtRhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Chart - 16 Average Rented Bike Monthly"
      ],
      "metadata": {
        "id": "W1AmOuJI4yaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15,9))\n",
        "sns.pointplot(data=df,x='hour',y='rented_bike_count',hue='month',ci= None, )\n",
        "ax.set_xlabel('Hour',fontsize=15)\n",
        "ax.set_ylabel('Rented_Bikes_Count',fontsize=15)\n",
        "ax.set_title('Average Rented bikes monthly' , fontsize=15)"
      ],
      "metadata": {
        "id": "K3OCQ1SQDvTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Jg8gA-Lz5dfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this analysis, we utilized a line plot to visualize the trends in rented bike count over the hours of the day, specifically focusing on the monthly patterns. Line plots are a popular choice for time-series data visualization, as they represent data points connected by lines, allowing us to observe the changes in data values over time."
      ],
      "metadata": {
        "id": "g7v-dwfw5e0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "T7DB1CgV5io8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of the graph reveals that the demand for rented bikes is lower during the winter months, specifically December, January, and February, in contrast to the summer months of May, June, and July, which exhibit the highest demand. Additionally, the graph indicates a significant surge in rented bike usage between 8:00 a.m. and 9:00 p.m., highlighting the preference of individuals to rent bikes during peak hours, likely for their daily commute to work. "
      ],
      "metadata": {
        "id": "To4xBbCR5jj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "z3kXX1QC5mcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight can help businesses tailor their strategies to maximize the growth potential during the summer months and peak hours, while also overcoming the challenge of lower demand during the winter season.\n",
        "\n",
        "The peak hours during non-holiday days present a significant opportunity for business growth, whereas holidays can result in a sharp decline in profitability."
      ],
      "metadata": {
        "id": "PrO8xITbv2DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "b5utGJhi5LeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Chart - 17 - Hourly demand of bike based on seasons\n"
      ],
      "metadata": {
        "id": "InIpYpB946H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15,9))\n",
        "sns.pointplot(data=df,x='hour',y='rented_bike_count',hue='seasons', palette=('Oranges'), ci=None )\n",
        "ax.set_xlabel('Hour',fontsize=15)\n",
        "ax.set_ylabel('Rented_Bikes_Count',fontsize=15)\n",
        "ax.set_title('Hourly demand of Bike based on season' , fontsize=15)"
      ],
      "metadata": {
        "id": "Pv1xr13L1_MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Line plots are a popular choice for time-series data visualization, as they represent data points connected by lines, allowing us to observe the changes in data values over time.\n",
        " we have plotted this line plot to analyze the hourly demand of rented bike over the seasons "
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazingly, it has been shown that during certain seasons, consumers prefer to rent bikes more frequently. Summer is the season with the most and winter is least number of rented bikes."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It is evident from the graph that there is a high demand for rented bikes during the summer months, followed by autumn and spring, and a comparatively lower demand during winter. we can conclude that the business has the potential to yield higher profits during the summer, autumn, and spring months, and comparatively lower profits during winter. By utilizing this information, businesses can develop effective strategies to maximize their profits"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multivariate Analysis"
      ],
      "metadata": {
        "id": "6AbaMAty7MyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's examine the heat map's correlation with each numerical feature to learn more about multilinearity."
      ],
      "metadata": {
        "id": "is_WAA-peGm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 18 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()['rented_bike_count']"
      ],
      "metadata": {
        "id": "Dyx_X1MEeqVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(18,12))\n",
        "corr = sns.heatmap(df.corr(),cmap='PiYG', square=True,annot=True)\n",
        "corr.set_xticklabels(corr.get_xticklabels(),horizontalalignment='right',  rotation=50 )\n",
        "   "
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When there are numerous variables or observations for each unit or person, the analysis of the data is referred to be multivariate."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature(°C) and dew point temperature(°C) columns of this graph demonstrate multicollinearity, as can be seen."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 19 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ = df.columns"
      ],
      "metadata": {
        "id": "XTo-V37kAQUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df,palette=\"bright\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I utilised pair plot to analyse data patterns and relationships between features. Pair plot is used to identify the optimal set of features to describe a relationship between two variables or to generate the most isolated clusters."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above graph showed less linear correlations between variables and non-linear separability of data points, which was new to me. Hence, we may conclude that both positive and negative trends are present in the relationship between each column in the graph."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "cQvq2u_JPbsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"variables\"] = X.columns\n",
        "  vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "  return(vif)"
      ],
      "metadata": {
        "id": "x-xzY9qVrZaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['rented_bike_count','dew_point_temp']]])"
      ],
      "metadata": {
        "id": "chybCna5rrYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"As a result, we may remove the DPT column from the dataset because\n",
        "   having two variables that are this highly\n",
        "   correlated won't improve prediction accuracy \n",
        "   and will instead make the model more complex.\"\"\"\n",
        "\n",
        "df.drop(columns=['dew_point_temp'],inplace=True)"
      ],
      "metadata": {
        "id": "vsbUmry4sNui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) - **Null Hypothesis** : There is no relation between Wind Speed and Rented Bike Count.\n",
        "\n",
        "**Alternate Hypothesis** : There is a relationship between Wind Speed and Rented Bike Count\n",
        "\n",
        "2) **Null Hypothesis** : There is no relation between Temperature and Rented Bike Count.\n",
        "\n",
        "**Alternate Hypothesis** : There is a relationship between Temperature and Rented Bike Count\n",
        "\n",
        "3) - **Null Hypothesis** : There is no relation between Holiday and Rented Bike Count.\n",
        "\n",
        "**Alternate Hypothesis** : There is a relationship between Holiday and Rented Bike Count"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Null Hypothesis** - There is no relation between Wind Speed and Rented Bike Count.\n",
        "\n",
        "**Alternate Hypothesis** - There is a relationship between Wind Speed and Rented Bike Count"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "def hypothesis_test(x, y, alpha=0.05):\n",
        "    stat, p = pearsonr(x, y)\n",
        "    print(f\"Correlation coefficient: {stat:.3f}, p-value: {p:.3f}\")\n",
        "    if p > alpha:\n",
        "        print(\"Fail to reject the null hypothesis\")\n",
        "    else:\n",
        "        print(\"Reject the null hypothesis\")\n",
        "\n",
        "first_sample = df[\"wind_speed\"].head(100)\n",
        "second_sample = df[\"rented_bike_count\"].head(100)\n",
        "\n",
        "hypothesis_test(first_sample, second_sample)\n"
      ],
      "metadata": {
        "id": "aujDdiq9NNt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We calculated the P-Value and Pearson Correlation coefficient values using the Pearson Correlation test.**"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To identify the connection between the testing series.  Here, we can see that the results of a statistical test comparing wind speed and the number of rented bikes show that the number of rented bikes does not depend on the wind speed, indicating that the two variables have no relationship.**"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) **Null Hypothesis** : There is no relation between Temperature and Rented Bike Count.\n",
        "\n",
        "**Alternate Hypothesis** : There is a relationship between Temperature and Rented Bike Count"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "def hypothesis_test(x, y, alpha=0.05):\n",
        "    stat, p = pearsonr(x, y)\n",
        "    print(f\"Correlation coefficient: {stat:.3f}, p-value: {p:.3f}\")\n",
        "    if p > alpha:\n",
        "        print(\"Fail to reject the null hypothesis\")\n",
        "    else:\n",
        "        print(\"Reject the null hypothesis\")\n",
        "\n",
        "first_sample = df[\"temperature\"].head(100)\n",
        "second_sample = df[\"rented_bike_count\"].head(100)\n",
        "\n",
        "hypothesis_test(first_sample, second_sample)\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We calculated the P-Value and Pearson Correlation coefficient values using the Pearson Correlation test.**"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To identify the connection between the testing series.  Here, we can see that the results of a statistical test comparing temperature and the number of rented bikes show that the number of rented bikes depend on the temperature, indicating that the two variables are correlated.**"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis** : There is no relation between Holiday and Rented Bike Count.\n",
        "\n",
        "**Alternate Hypothesis** : There is a relationship between Holiday and Rented Bike Count"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "def hypothesis_test(x, y, alpha=0.05):\n",
        "    stat, p = pearsonr(x, y)\n",
        "    print(f\"Correlation coefficient: {stat:.3f}, p-value: {p:.3f}\")\n",
        "    if p > alpha:\n",
        "        print(\"Fail to reject the null hypothesis\")\n",
        "    else:\n",
        "        print(\"Reject the null hypothesis\")\n",
        "\n",
        "first_sample = df[\"holiday\"].head(100)\n",
        "second_sample = df[\"rented_bike_count\"].head(100)\n",
        "\n",
        "hypothesis_test(first_sample, second_sample)\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We calculated the P-Value and Pearson Correlation coefficient values using the Pearson Correlation test.**"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To identify the connection between the testing series.  Here, we can see that the results of a statistical test comparing holiday and the number of rented bikes show that the number of rented bikes depend on the holiday, indicating that the two variables are correlated.**"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "EYfZwFEZufbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As null values have previously been handled with, our dataset is complete and free of duplicate, missing, or null values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's use the IQR method to create a function for the outlier treatment, capping the outliers in the 25–75 percentile."
      ],
      "metadata": {
        "id": "F888CW7_5EhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "WhRD5ystQm2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numeric_features: \",continuous_variable)\n",
        "print(\"Categorical_features: \",categorical_features)"
      ],
      "metadata": {
        "id": "NX7-gn7MvO9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatment\n",
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20,10))\n",
        "for i, ax in zip(continuous_variable, axes.flatten()):\n",
        "    sns.boxplot(df[i], ax=ax)\n",
        "    ax.set_title(i)\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "rsfiaqjUmPeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can easily see from the box plots above that the variables \"wind speed,\" \"rainfall,\" \"snowfall,\" and \"solar radiation\" have a number of outliers, whereas the other features are good because they are Numeric in nature.**"
      ],
      "metadata": {
        "id": "CyjODFHVq7Jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's build a function to determine the percentage and quantity of outliers present in each feature so that we can manage them appropriately.**"
      ],
      "metadata": {
        "id": "R0HekXqe8FFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##using IQR to define the code for outlier detection and percentage.\n",
        "def detect_outliers(bike_df):\n",
        "    data = sorted(bike_df)\n",
        "    q1, q2, q3 = np.percentile(data, [25, 50, 75])\n",
        "    print(f\"q1:{q1}, q2:{q2}, q3:{q3}\")\n",
        "    IQR = q3 - q1\n",
        "    lower_bound, upper_bound = q1 - 1.5*IQR, q3 + 1.5*IQR\n",
        "    print(f\"Lower bound: {lower_bound}, Upper bound: {upper_bound}, IQR: {IQR}\")\n",
        "    \n",
        "\n",
        "    outliers = [i for i in data if i < lower_bound or i > upper_bound]\n",
        "    num_outliers = len(outliers)\n",
        "    perc_outliers = round(num_outliers * 100 / len(data), 2)\n",
        "    print(f\"Total number of outliers are: {num_outliers}\")\n",
        "    print(f\"Total percentage of outlier is: {round(perc_outliers*100/len(data),2)} %\")\n",
        "    \n",
        "\n",
        "    results = (\n",
        "        q1, q2, q3,\n",
        "        IQR,\n",
        "        lower_bound, upper_bound,\n",
        "        outliers,\n",
        "        num_outliers,\n",
        "        perc_outliers\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "85L48NvrulIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the IQR, lower and upper bounds, and counting the number of outliers present in each continuous numerical feature\n",
        "for feature in continuous_variable:\n",
        "  print(feature,\":\")\n",
        "  detect_outliers(df[feature])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "UQ9ukJ-H3OQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Using the IQR technique to define the function that treat outliers\n",
        "def treat_outliers_iqr(data):\n",
        "    # Sort the data in ascending order\n",
        "    sorted_data = sorted(data)\n",
        "    \n",
        "    # Calculate the quartile indices\n",
        "    q1_index = int(len(sorted_data) * 0.25)\n",
        "    q2_index = int(len(sorted_data) * 0.5)\n",
        "    q3_index = int(len(sorted_data) * 0.75)\n",
        "    \n",
        "    \n",
        "    # Calculate the quartile values\n",
        "    q1 = sorted_data[q1_index]\n",
        "    q2 = sorted_data[q2_index]\n",
        "    q3 = sorted_data[q3_index]\n",
        "    \n",
        "    # Calculate the interquartile range (IQR)\n",
        "    iqr = q3 - q1\n",
        "    \n",
        "    # Identify the outliers\n",
        "    lower_bound = q1 - (1.5 * iqr)\n",
        "    upper_bound = q3 + (1.5 * iqr)\n",
        "    outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
        "    \n",
        "    # Treat the outliers (e.g., replace with the nearest quartile value)\n",
        "    treated_data = [q1 if x < lower_bound else q3 if x > upper_bound else x for x in data]\n",
        "    treated_data_int = [int(absolute) for absolute in treated_data]\n",
        "    \n",
        "    return treated_data_int\n"
      ],
      "metadata": {
        "id": "4iQxPnOcB063"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Using the function we defined previously to treat outliers, passing each feature one by one from the continuous value feature list.\n",
        "for treat in continuous_variable:\n",
        "  df[treat]= treat_outliers_iqr(df[treat])"
      ],
      "metadata": {
        "id": "YM0vlZGB-KqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 4, figsize=(32, 10))\n",
        "for col, ax in zip(continuous_variable, axes.flatten()):\n",
        "    sns.boxplot(df[col], ax=ax)\n",
        "    ax.set_title(col.title(), weight='bold')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "TRypqWqgBQUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bivariate outlier analysis "
      ],
      "metadata": {
        "id": "rDgDlk5P4LZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['hour','seasons', 'holiday', 'functioning_day', 'month', 'weekday', 'year']"
      ],
      "metadata": {
        "id": "veJQqNBu4Jpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the outliers present in each categorical_features.\n",
        "for cat_f in categorical_features:\n",
        "  plt.figure(figsize=(12,6))\n",
        "  sns.boxplot(x = cat_f ,y = rented_bike_count[0],data=df, )\n",
        "  plt.title(cat_f)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JdaDgOV1Tuc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the IQR, lower and upper bounds, and the number of outliers present in each category of object dtype characteristics\n",
        "for outlier in categorical_features:\n",
        "    print(f\"Feature: {outlier}\")\n",
        "    cats = df[outlier].unique().tolist()\n",
        "    for i, (cat, data) in enumerate(df.groupby(outlier)[\"rented_bike_count\"]):\n",
        "        print(f\"{i+1}: Category: {cat}\")\n",
        "        detect_outliers(data)\n",
        "        print()"
      ],
      "metadata": {
        "id": "m_dGUeV26YE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Despite the fact that the dataset contains a few categorical outliers, we won't treat them because the ML model and algorithm we'll be using to handle categorical outliers can do so without compromising the accuracy of the model.\n"
      ],
      "metadata": {
        "id": "eaT_Fzac-JKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NDpt4ag3-EVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our two distinct functions—one for \"outlier detection\" and the other for \"outlier treatment using IQR\"—have been defined, and all observations of continuous characteristics have been run through them. Extreme left outliers (25%) and extreme right outliers (>75%) in the 25th and 75th quartile values have been successfully eliminated."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a variety of encoding methods, but the main ones are:"
      ],
      "metadata": {
        "id": "kcP0GniRM5Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1)- When features are ordinal in nature and have a rank between them, ordinal encoding is used.\n",
        "\n",
        "2)- When the features are nominal in nature and have equal weight, nominal encoding is used.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Z5L7XzFN6Xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will utilise One-Hot Encoding (Type of Nominal encoding) in our scenario because all of our category columns are nominal in nature:"
      ],
      "metadata": {
        "id": "vpkOe-cCQzTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "DAC5x8MFkukh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_functioning_day'] = (df['functioning_day'] == 'Yes').astype(int)\n",
        "df['Not functioning_day'] = (df['functioning_day'] == 'No').astype(int)\n",
        "\n",
        "# Drop the original 'Functioning Day' column\n",
        "df.drop(columns=['functioning_day'], inplace=True)\n"
      ],
      "metadata": {
        "id": "UIiQP3e7n6HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of the season names\n",
        "# Encode your categorical columns of Season \n",
        "season_names = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
        "\n",
        "# Loop through the season names and create a new column for each season\n",
        "for season in season_names:\n",
        "    df[season] = (df['seasons'] == season).astype(int)\n",
        "\n",
        "# Drop the original 'Seasons' column\n",
        "df.drop(columns=['seasons'], inplace=True)\n"
      ],
      "metadata": {
        "id": "RC0DaIy_jRdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns of Holiday\n",
        "df['is_holiday '] = np.where(df['holiday']=='holiday',1,0)\n",
        "df['No holiday'] = np.where(df['holiday']==' No holiday',1,0)\n",
        "\n",
        "# Drop the 'holiday' column from the dataframe\n",
        "df.drop(columns=['holiday'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "AWnUZUxLleib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head().T"
      ],
      "metadata": {
        "id": "hpC7z6z_n4bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Encode your categorical columns of Hour, Month, Weekday and year\n",
        "category=['hour','month','weekday', 'year']\n",
        "for col in category:\n",
        "  df[col]=df[col].astype('category')"
      ],
      "metadata": {
        "id": "LlOGBAcCm90M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dummy variable for ease of operations on categorical features.\n",
        "df = pd.get_dummies(df,drop_first=True,sparse=True)\n",
        "df.head().T\n",
        "     "
      ],
      "metadata": {
        "id": "3TS9dFi5kAZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "jVRvlt0npobw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "aNFwQhrYKZMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In order to make our categorical object type characteristics suitable to be fed into multiple ML algorithms in the future, we utilized the one-hot encoding technique to convert their dummy into int type.\n",
        "\n",
        "- Because each of the category features has 3–4 distinct orderless categories (which is less in number). Hence, rather of using an ordinal encoding method, nominal encoding is recommended.\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking which of the variables are continous in nature\n",
        "for i in df.columns:\n",
        "  print(f\"The total number of unique counts in {i} is: {df[i].nunique()}\")"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df.columns"
      ],
      "metadata": {
        "id": "W76N-VK154Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "df['rented_bike_count']=np.log1p(df['rented_bike_count'])"
      ],
      "metadata": {
        "id": "x7YrIQf-7cC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing StandardScaler and normalize library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "##scaler = StandardScaler()\n",
        "from sklearn.preprocessing import normalize"
      ],
      "metadata": {
        "id": "v5L9-a5z-Drz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "X = df.drop(columns = ['rented_bike_count'] , axis = 1)\n",
        "y = df['rented_bike_count']"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train , X_test, y_train, y_test =train_test_split(X, y, test_size= 0.2 , random_state =0)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting shape\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "2BvDsElyGMEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_scalar = StandardScaler()\n",
        "X_train = std_scalar.fit_transform(X_train)\n",
        "X_test = std_scalar.transform(X_test)\n"
      ],
      "metadata": {
        "id": "AWHgYbETHbHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_metrics(y_train_actual, y_train_pred, y_test_actual, y_test_pred): \n",
        "   \n",
        "    # Calculate mean absolute error for train and test sets\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    \n",
        "    # Print MAE values\n",
        "    print(f\"MAE on train set: {train_mae:.3f}\")\n",
        "    print(f\"MAE on test set: {test_mae:.3f}\")\n",
        "\n",
        "\n",
        "     # Calculate mean squared error for train and test sets\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    \n",
        "    # Print MSE values\n",
        "    print(f\"MSE on train set: {train_mse:.3f}\")\n",
        "    print(f\"MSE on test set: {test_mse:.3f}\")\n",
        "\n",
        "\n",
        "     # Calculate root_mean_squared_error for train and test sets\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    \n",
        "    # Print RMSE values\n",
        "    print(f\"RMSE on train set: {train_rmse:.3f}\")\n",
        "    print(f\"RMSE on test set: {test_rmse:.3f}\")\n",
        "\n",
        "\n",
        "    # Calculate r2_score for train and test sets\n",
        "    R2_train= r2_score(y_train,y_train_pred)\n",
        "    R2_test= r2_score(y_test,y_test_pred)\n",
        "    print(\"R2 on train set:\" ,R2_train)\n",
        "    print(\"R2 on test set:\" ,R2_test)\n",
        "\n",
        "    ## Adjusted R2_score\n",
        "    train_Adj_R2 = (1-(1-r2_score(y_train, y_train_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "    print( 'Adjusted R2 on train is :', train_Adj_R2)\n",
        "    test_Adj_R2 = (1-(1-r2_score(y_test, y_test_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "    print( 'Adjusted R2 on test is :', test_Adj_R2)"
      ],
      "metadata": {
        "id": "5BhCazzuMTXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1  Linear regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "regressor= LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "regressor.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_Lr=regressor.predict(X_train)\n",
        "y_pred_test_Lr=regressor.predict(X_test)\n",
        "     "
      ],
      "metadata": {
        "id": "XB9_BlZ9LkGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.coef_"
      ],
      "metadata": {
        "id": "pbJuyz0aBPfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.intercept_"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "4EU-f7crC2N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_test_Lr)"
      ],
      "metadata": {
        "id": "aIrS2XGVDphA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the regression metrics\n",
        "regression_metrics(y_train,y_pred_train_Lr,y_test,y_pred_test_Lr)"
      ],
      "metadata": {
        "id": "Wejbpz-oFRJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_pred_test_Lr[:100], label='Predicted')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FP-AitMZ3zb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Linear Regression, the most fundamental and simplest machine learning model, is where we started. In order to complete our ML model, we have made an effort to assess the most significant regression metrics on both the train and test data sets. The fact that both r2 scores are quite close in this case for linear regression explains why our model performed correctly on the test dataset.\n",
        "\n",
        "#####We understand that the greatest r2 score obtained in the implementation of the LR model was 0.84 for the 'dependent' and 'independent' variables and y."
      ],
      "metadata": {
        "id": "AVgeqshKSAcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Ridge (L2) Regression**"
      ],
      "metadata": {
        "id": "YXK9HYn2hra5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Create a Ridge instance\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define a dictionary of parameter values to be searched\n",
        "param_grid_ridge = {\"alpha\": [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100], \"max_iter\":[1,2,3]}\n",
        "\n",
        "# Create a GridSearchCV object with the Ridge instance and the parameter grid\n",
        "grid_search = GridSearchCV(ridge, param_grid_ridge, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best estimator (Ridge model) from the GridSearchCV object\n",
        "best_ridge = grid_search.best_estimator_\n",
        "\n",
        "# Use the best estimator to predict the output for the training and test data\n",
        "y_train_ridge_predict = best_ridge.predict(X_train)\n",
        "y_test_ridge_predict = best_ridge.predict(X_test)\n",
        "\n",
        "# Print the best alpha value found by GridSearchCV and the corresponding negative mean squared error\n",
        "print(\"Best parameter values : \", grid_search.best_params_)\n",
        "print(\"Negative mean squared error: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "V5SP6vysWV6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge's regression metrics are calculated.\n",
        "regression_metrics(y_train,y_train_ridge_predict,y_test,y_test_ridge_predict)"
      ],
      "metadata": {
        "id": "KCqDnnf1Yfpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_ridge_predict[:100], label='Predicted' ,color='blue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='red')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JsSiCj9KbILp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Lasso Regression** (L1)"
      ],
      "metadata": {
        "id": "ipkyW396c9Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a lasso instance\n",
        "lasso= Lasso()\n",
        "\n",
        "# Define a dictionary of parameter values to be searched\n",
        "param_grid_lasso = {\"alpha\": [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014] , \"max_iter\":[7,8,9,10]} \n",
        "\n",
        "# Create a GridSearchCV object with the lasso instance and the parameter grid\n",
        "grid_search = GridSearchCV(lasso, param_grid_lasso, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best estimator (lasso model) from the GridSearchCV object\n",
        "best_lasso = grid_search.best_estimator_\n",
        "\n",
        "# Use the best estimator to predict the output for the training and test data\n",
        "y_train_lasso_predict = best_lasso.predict(X_train)\n",
        "y_test_lasso_predict = best_lasso.predict(X_test)\n",
        "\n",
        "# Print the best alpha value found by GridSearchCV and the corresponding negative mean squared error\n",
        "print(\"Best parameter values : \", grid_search.best_params_)\n",
        "print(\"Negative mean squared error: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "4aM11fI8c4sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lasso's regression metrics are calculated.\n",
        "regression_metrics(y_train,y_train_lasso_predict,y_test,y_test_lasso_predict)\n",
        "     "
      ],
      "metadata": {
        "id": "pPRcjWP5h0VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_lasso_predict[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R5KR2m-zieQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Elastic Net Regression**"
      ],
      "metadata": {
        "id": "CvkMUuIEIiHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Create a elestic_net  instance\n",
        "elastic_net = ElasticNet()\n",
        "\n",
        "# Define a dictionary of parameter values to be searched\n",
        "param_grid_E_net = {\"alpha\": [1e-5,1e-4,1e-3,1e-2,1,5]} \n",
        "\n",
        "# Create a GridSearchCV object with the elestic_net instance and the parameter grid\n",
        "grid_search = GridSearchCV(elastic_net, param_grid_E_net, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best estimator (Elestic_net model) from the GridSearchCV object\n",
        "best_elastic_net = grid_search.best_estimator_\n",
        "\n",
        "# Use the best estimator to predict the output for the training and test data\n",
        "y_train_elestic_net_predict = best_elastic_net.predict(X_train)\n",
        "y_test_elestic_net_predict = best_elastic_net.predict(X_test)\n",
        "\n",
        "# Print the best alpha value found by GridSearchCV and the corresponding negative mean squared error\n",
        "print(\"Best parameter values : \", grid_search.best_params_)\n",
        "print(\"Negative mean squared error: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "pqqU6KYlhBVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ElasticNet regression metrics are calculated.\n",
        "regression_metrics(y_train,y_train_elestic_net_predict,y_test,y_test_elestic_net_predict)"
      ],
      "metadata": {
        "id": "3egzR1DykrQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_elestic_net_predict[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jsmEaCVjET0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to its ability to use all feasible combinations of hyperparameters and their values, GridSearchCV was chosen as the hyperparameter optimization technique. The optimal value for the hyperparameters is then chosen after calculating the performance for each combination. The most precise tuning approach is provided by this."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso, Ridge, and Elastic net models were all used, but we still weren' able to detect any appreciable change in the r2 score, MSE, or MAPE."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2   Decision Trees"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the decision tree model for training \n",
        "DT_Regressor = DecisionTreeRegressor(max_depth=10,  max_leaf_nodes=100)\n",
        "\n",
        " # Fitting Decision Tree to the Training set\n",
        "DT_Regressor.fit(X_train, y_train)\n",
        "\n",
        "#y pred for test and train data\n",
        "y_train_predict_DT = DT_Regressor.predict(X_train)\n",
        "y_test_predict_DT = DT_Regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "wRn9vtWSEnX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "regression_metrics(y_train,y_train_predict_DT,y_test,y_test_predict_DT)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_predict_DT[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kYshoSixUf5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric \n",
        "\n",
        "Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the use of LR models, we tried \"Decision Tree\" and observed a great increase in the r2 score from 0.84 to 0.85, indicating that \"90% Variance of our Test Dataset is Covered by Our Training Model,\" which is excellent. It's great that our RMSE decreased and moved below 5 (=0.631) on the other side."
      ],
      "metadata": {
        "id": "0qWy3jitRUjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tried the \"Decision Tree\" after applying the linear regression model, and we noticed that the r2 score has gone up by 1%, from.84 to.85, which equals '85.%'. As the trained model successfully captured the variation in our test data, we choose to fine-tune the hyperparameters and evaluate the outcomes."
      ],
      "metadata": {
        "id": "fEbGQKaaTkGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "decision Tree"
      ],
      "metadata": {
        "id": "C8uBfuYjQqaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the range of hyperparameters\n",
        "param_grid = {\n",
        "    'max_depth': [8, 9, 10],\n",
        "    'min_samples_leaf': [6, 7, 8],\n",
        "    'min_samples_split': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# Create an instance of the model\n",
        "Decision_Tree = DecisionTreeRegressor()\n",
        "\n",
        "# Define the grid search object\n",
        "grid_search_Tree = GridSearchCV(\n",
        "    estimator=Decision_Tree,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "grid_search_Tree.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and negative mean squared error\n",
        "print(f\"The best hyperparameters are: {grid_search_Tree.best_params_}\")\n",
        "print(f\"Negative mean squared error is: {grid_search_Tree.best_score_}\")\n",
        "\n",
        "# Make predictions on the training and testing data\n",
        "y_train_DecisionTree_pred = grid_search_Tree.predict(X_train)\n",
        "y_test_DecisionTree_pred = grid_search_Tree.predict(X_test)\n"
      ],
      "metadata": {
        "id": "W71EHEWJp6Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "regression_metrics(y_train,y_train_DecisionTree_pred,y_test,y_test_DecisionTree_pred)"
      ],
      "metadata": {
        "id": "QRDHh362oeWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_DecisionTree_pred[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xZOfLcYkstmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As GridSearchCV uses all feasible combinations of hyperparameters and yields more precise results, we chose it as our chosen hyperparameter optimization strategy. In order to choose the best value for the hyperparameters, it then evaluates the performance for each combination. This tuning approach provides the best results."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the r2 score with the lowest MSE and the optimum value for our situation, we have tried various parameter combinations. It was determined that the following combination worked best: \"max depth\": \"8,9, 10,\" \"min samples leaf\": \"6, 7, 8.\" The MSE on the test dataset was improved by'min samples split':[1, 2, 3, 4] via hyperparameter tuning of Decision trees, from 41% to 36%."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We measure the performance of our ML model using several measures in an effort to minimise the gaps between real and predicted values. These measurements all attempt to indicate our level of satisfaction with the actual or expected output. In this instance, there is little to no variation between the train and test data for each evaluation metric, indicating that our model accurately predicts the predicted result. As a result, the dependent variable, the number of rented bikes, which has an impact on the business, is correctly predicted to an extent of 86.9%, and 3% off the mean of the actual absolute values."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 Random Forest"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#importing the decision tree model for training \n",
        "RF_Regressor = RandomForestRegressor(n_estimators=100, max_depth=15)\n",
        "\n",
        " # Fitting Decision Tree to the Training set\n",
        "RF_Regressor.fit(X_train, y_train)\n",
        "\n",
        "#y pred for test and train data\n",
        "y_RandomForest_train_pred = RF_Regressor.predict(X_train)\n",
        "y_RandomForest_test_pred = RF_Regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "zMHiwZw77E3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train,y_RandomForest_train_pred,y_test,y_RandomForest_test_pred)"
      ],
      "metadata": {
        "id": "icgU78a_8_RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_RandomForest_test_pred[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the implementation of our third model, the Random Forest, our predictions are sprouting like a well-tended garden. The training dataset yields a magnificent r2 score of 0.95, while the test dataset follows closely behind with a score of 0.91 , we've pruned the MSE from 0.361 to a trim  0.232, shaping our model towards optimal model.. It's a pleasure to witness our efforts blossom into such fruitful results!"
      ],
      "metadata": {
        "id": "CjyLbOj2DnCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RandomizedSearchCV**"
      ],
      "metadata": {
        "id": "y6ew8YfRk5vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Instantiate the Random Forest model\n",
        "RF_tree = RandomForestRegressor()\n",
        "\n",
        "# Define the parameters to be tuned\n",
        "param_dist = {'n_estimators': [100], 'max_depth': [15,17,19], 'min_samples_leaf': [1, 2,3]} \n",
        "\n",
        "# Perform hyperparameter tuning using RandomizedSearchCV\n",
        "RF_treeR = RandomizedSearchCV(estimator=RF_tree, param_distributions=param_dist, n_iter=5, n_jobs=-1, scoring='neg_mean_squared_error', cv=3, verbose=3)\n",
        "RF_treeR.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on training and test datasets\n",
        "y_train_RFtree_pred = RF_treeR.predict(X_train)\n",
        "y_test_RFtree_pred = RF_treeR.predict(X_test)\n",
        "\n",
        "# Retrieve the best parameters and negative mean square error score\n",
        "best_params = RF_treeR.best_params_\n",
        "best_score = RF_treeR.best_score_\n",
        "\n",
        "# Print the best parameters and negative mean square error score\n",
        "print(f\"The optimal hyperparameters found were: {best_params}\")\n",
        "print(f\"The negative mean square error score is: {best_score}\")\n"
      ],
      "metadata": {
        "id": "2OXBWat3H2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_RFtree_pred,y_test,y_test_RFtree_pred)"
      ],
      "metadata": {
        "id": "OYuu3HwJJqmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_RandomForest_test_pred[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wrWp9rHdOHo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given our large dataset and the fact that it works well for large, complicated models where we simply want to choose random parameters from a bag of parameters, RandomizedSearchCV has been utilized in Random Forest. By using randomly selected subsets of the specified parameters, it shortens the processing and training times without sacrificing the model's accuracy."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After exploring with multiple hyperparameters using RandomizedSearchCV, we found that there was not much of a noticeable improvement. Although the MSE on the test dataset was decreased from 0.232 to 0.212 and the r2 score was also raised by 1% from 91% to 92%"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model- 4 - GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "NvT-BlgupoFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the regressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "  \n",
        "# Create a regressor object\n",
        "GBR = GradientBoostingRegressor(max_depth=4, \n",
        "                                n_estimators=500, \n",
        "                                learning_rate=0.1,\n",
        ") \n",
        "  \n",
        "# Fit the regressor with X and Y data\n",
        "GBR.fit(X_train, y_train)\n",
        "\n",
        "# Predict with the model\n",
        "y_train_GBR_pred = GBR.predict(X_train)\n",
        "y_test_GBR_pred = GBR.predict(X_test)\n"
      ],
      "metadata": {
        "id": "eOdXK2xNtu7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics \n",
        "regression_metrics(y_train,y_train_GBR_pred,y_test,y_test_GBR_pred)"
      ],
      "metadata": {
        "id": "YJTq5yMkvimi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "TqxIQ3MydeNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_GBR_pred[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Akc3iCRwwptC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "uILZ8Nsvw-EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GradientBoostingRegressor with  RandomizedSearchCV"
      ],
      "metadata": {
        "id": "n162prxNxFLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating XGBoost instance\n",
        "Gbm = GradientBoostingRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters_GBM={\"max_depth\":[10],\"learning_rate\":[0.01,0.1], \"n_estimators\":[30,40,50]}\n",
        "\n",
        "# Train the model\n",
        "Gbm_R= RandomizedSearchCV(Gbm,parameters_GBM,scoring='neg_mean_squared_error',n_jobs=-1,cv=3,verbose=3)\n",
        "Gbm_R.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_GBR_rand_pred = Gbm_R.predict(X_train)\n",
        "y_test_GBR_rand_pred = Gbm_R.predict(X_test)  \n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {Gbm_R.best_params_}\")\n",
        "print(f\"Negative mean square error is: {Gbm_R.best_score_}\")"
      ],
      "metadata": {
        "id": "yyEbDSJ4xAab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_GBR_rand_pred,y_test,y_test_GBR_rand_pred)"
      ],
      "metadata": {
        "id": "hC-gIP4jyXP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_GBR_rand_pred[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pzmiv2uRU0l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which hyperparameter optimization technique have you used and why?**\n",
        "\n"
      ],
      "metadata": {
        "id": "_MEcc2YiXLzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even yet, RandomizedSearchCV was still the preferable choice because it requires extremely little processing time while maintaining accuracy. So, it was determined by both of us to employ that hyperparameter optimization method."
      ],
      "metadata": {
        "id": "ke-32acOW95e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "9uUyKb-VXDty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our GBM model was improved by experimenting with a number of parameters, and the results revealed that it is optimised and neither underfitting nor overfitting, with r2 scores of 0.98 on the training dataset and 0.92 on the testing set. The optimization produced the ideal values for the following parameters: \"n estimators\": [500, 600], \"max depth\": [3,4,5], and \"learning rate\": [0.01, 0.1]."
      ],
      "metadata": {
        "id": "NtKnsjkFZYAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###XGboost (Extreme Gradient Boost)"
      ],
      "metadata": {
        "id": "topC2kzWvmdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# define hyperparameters\n",
        "params = {\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 8,\n",
        "    'n_estimators': 100\n",
        "}\n",
        "\n",
        "# create XGBoost regressor object\n",
        "xgb_model = xgb.XGBRegressor(**params)\n",
        "\n",
        "# train model on training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# predict target variable for training and testing data\n",
        "y_train_xgb_pred = xgb_model.predict(X_train)\n",
        "y_test_xgb_pred = xgb_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "FfA7qV6yGkF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_metrics(y_train,y_train_xgb_pred,y_test,y_test_xgb_pred)"
      ],
      "metadata": {
        "id": "cmegdCjT5iSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "HaciB6qnLnfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_xgb_pred[:100], label='Predicted' ,color='skyblue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QPFZhVeQL5_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient boosting method known as XGBoost (eXtreme Gradient Boosting) is well known for its high accuracy. We've utilized XGBoost."
      ],
      "metadata": {
        "id": "9t8GTWAoL4RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Our model achieved excellent and improved R2 scores of 0.97 for the training dataset and 0.93 for the testing dataset, indicating high accuracy and potential benefits for a business such as better decision-making, improved customer satisfaction, and cost savings. Moreover, with a reduced MSE, our model's predictions are more accurate, which further enhances its potential benefits. Excited to further improve our model's efficiency, we have decided to tune its various hyperparameters using xgboost.\""
      ],
      "metadata": {
        "id": "j4FKmbhRQl5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Cross- Validation & Hyperparameter Tuning**\n",
        "\n",
        "XGBOOST"
      ],
      "metadata": {
        "id": "-xHB3bLQRJsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgb_model_CV = XGBRegressor()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "parameters = {\n",
        "    \"learning_rate\": [0.01, 0.1],\n",
        "    \"max_depth\": [5,8]\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search_CV = GridSearchCV(\n",
        "    xgb_model_CV, \n",
        "    parameters, \n",
        "    scoring='neg_mean_squared_error', \n",
        "    n_jobs=-1, \n",
        "    cv=5, \n",
        "    verbose=3\n",
        ")\n",
        "grid_search_CV.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training and testing data\n",
        "y_train_XGBCV_pred = grid_search_CV.predict(X_train)\n",
        "y_test_XGBCV_pred = grid_search_CV.predict(X_test)\n",
        "\n",
        "# Print the best hyperparameters and negative mean squared error\n",
        "print(f\"Best hyperparameters found: {grid_search_CV.best_params_}\")\n",
        "print(f\"Negative mean squared error: {grid_search_CV.best_score_}\")\n"
      ],
      "metadata": {
        "id": "9Hdd7MS1SEl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_metrics(y_train,y_train_XGBCV_pred,y_test,y_test_XGBCV_pred)"
      ],
      "metadata": {
        "id": "_47ZCOg_TtGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#plotting evaluation graph\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "ax.plot(y_test_XGBCV_pred[:100], label='Predicted' ,color='blue')\n",
        "ax.plot(np.array(y_test[:100]), label='Actual', color='orange')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Om7UjVpbxqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which hyperparameter optimization technique have you used and why?**"
      ],
      "metadata": {
        "id": "S86370mdvMZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GridSearchCV, XGboost requires a lot of processing time due to its complexity. Hence, using GridSearchCV to tune hyperparameters proved to be a challenging effort for us. For this scenario, RandomizedSearchCV is a great hyperparameter optimization strategy."
      ],
      "metadata": {
        "id": "wxz9GZHRvIGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "SjLog4fz1WWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our XG Boost model has been tuned using a variety of parameters, resulting in an R2 score of 0.97 on the training dataset and 0.93 on the testing set. These scores suggest that our model has achieved an optimal balance between bias and variance, indicating that it is not underfitting or overfitting. The best parameters obtained by the optimatization is 'max_depth': [5,8], 'learning_rate':[ 0.01,0.1]}.\n",
        "\n",
        "\n",
        "In addition, we have observed a reduction in our mean squared error (MSE) values, which have reached a minimum of 0.177 - the lowest error rate achieved among all models. Additionally, our mean absolute error (MAE) values have also decreased.\n",
        "\n",
        "Moreover, we have found that increasing the max_depth of our decision trees causes our model to overfit the data, indicating that the best combination of hyperparameters has been achieved with the current set of values."
      ],
      "metadata": {
        "id": "E5PbiRgOwyVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)- This metric, known as MAE (Mean Absolute Error), determines the average size of predictions' errors without taking into account their direction. It bears an inverse relationship to the model's accuracy. Regression analysis aims to reduce the MAE, which will ultimately have a beneficial business impact.\n",
        "\n",
        "2)- MSE (Mean Squared Error) is a common metric used to evaluate the accuracy of a predictive model. It measures the average of the squared differences between the predicted values and the actual values.MSE is widely used in regression analysis because it penalizes larger errors more heavily than smaller ones, due to the squaring of the differences. A lower MSE indicates that the model is better at predicting the outcomes, as the squared differences between the predicted and actual values are smaller on average\n",
        "\n",
        "3)- RMSE (Root Mean Squared Error) is a commonly used metric to evaluate the accuracy of a regression model. It is the square root of the Mean Squared Error (MSE), which is the average of the squared differences between the predicted values and the actual values. RMSE is widely used in regression analysis because it provides a measure of the error in the same units as the original data. This makes it easy to interpret the magnitude of the error and compare it to the scale of the original data. \n",
        "\n",
        "4)- The R2 score, also known as the coefficient of determination, is a commonly used metric in regression analysis. It measures the proportion of the variance in the dependent variable that is explained by the independent variables.Analysts may quickly and readily evaluate a model's goodness of fit and compare several models using the R2 score, making it a useful tool. By giving a precise indication of how well the model explains the variance in the dependant variable, it enables analysts to choose the best model and conduct more research with confidence.\n",
        "\n",
        "5) - Adjusted R2 is a modified version of the R2 (coefficient of determination) metric that takes into account the number of independent variables in a regression model,  Adjusted R2 is a useful metric that provides a more accurate measure of the goodness of fit of a regression model than R2, as it takes into account the complexity of the model and the number of independent variables. It is especially useful when comparing models with different numbers of independent variables.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?\n",
        "\n"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results of all the models"
      ],
      "metadata": {
        "id": "28V_oSZcTUZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ML model summary for train datset** "
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "train = PrettyTable(['SL NO',\"MODEL_NAME\", \"Train MAE\", \"Train MSE\",\"Train RMSE\",'Train R^2','Train Adjusted R^2'])\n",
        "train.add_row(['1','Linear Regression','0.451','0.431','0.656',\"0.8238815033172513\",\" 0.8180628391200631\"])\n",
        "train.add_row(['2','Ridge Regression','0.451','0.430','0.656',\"0.8241217917636909\",\" 0.8183110662998365\"])\n",
        "train.add_row(['3','lasso Regression ','0.452','0.430','0.656',\"0.8240981731870523\",\" 0.8182866674044416\"])\n",
        "train.add_row(['4','Elastic_Net','0.451','0.430','0.656',\"0.8241226210806791\",'0.818311923016088'])\n",
        "train.add_row(['5','Decision Tree ','0.375','0.280','0.529',\"0.8855926397615665\",\"0.8818128095707982\"])\n",
        "train.add_row(['6','Random forest','0.161','0.059','0.243',\"0.9758434506469656\",'0.9750453581609657'])\n",
        "train.add_row(['7','GradientBoostingRegressor','0.147',' 0.045 ','0.211',\"0.9817521187026687\",' 0.9811381108904208'])\n",
        "train.add_row(['8','XGboost','0.149','0.052','0.228',\"0.9788153777902806\",'0.9781154728677176'])\n",
        "print(train)"
      ],
      "metadata": {
        "id": "NjeSrgKwe1bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ML model summary for test datset**"
      ],
      "metadata": {
        "id": "RFTnEPx-Od2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "test = PrettyTable(['SL NO',\"MODEL_NAME\", \"test MAE\", \"test MSE\",\"test RMSE\",'test R^2','test Adjusted R^2'])\n",
        "test.add_row(['1','Linear Regression','0.439','0.411','0.641',\"0.8465076552197833\",\" 0.8414365217049207\"])\n",
        "test.add_row(['2','Ridge Regression','0.439','0.411','0.641',\"0.8466522668216554\",\"0.8415859110352322\"])\n",
        "test.add_row(['3','lasso Regression ','0.440','0.411','0.641',\"0.8467750592305618\",\"0.841712760302486\"])\n",
        "test.add_row(['4','Elastic_Net','0.439','0.411','0.641',\"0.8466691741131821\",'0.8416033769157415'])\n",
        "test.add_row(['5','Decision Tree ','0.401','0.361','0.601',\"0.8652143693343365\",\"0.860761274751872\"])\n",
        "test.add_row(['6','Random forest','0.276','0.213','0.462',\"0.9203796422993189\",'0.917749117207143'])\n",
        "test.add_row(['7','GradientBoostingRegressor','0.260','0.195','0.441',\"0.9273355088408269\",'0.9248904816884816'])\n",
        "test.add_row(['8','XGboost','0.247','0.177','0.420',\"0.9340355547816962\",'0.9318561984794985'])\n",
        "print(test)"
      ],
      "metadata": {
        "id": "pt23xbdtDg4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the metrics we have got, it appears that the XGBoost model has the best performance among the models you have tested, with the lowest test RMSE and highest test R^2 and test adjusted R^2.\n",
        "\n",
        "\n",
        "We have chosen XGboost as our final prediction model with hyperparameters  {'learning_rate': 0.1, 'max_depth': 8} as it is very clear from above dataframe that it has given  the lowest test RMSE and highest test R^2  score(0.93) and test adjusted R^2 on the testing dataset among all other models."
      ],
      "metadata": {
        "id": "xnu66WpAH_LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost, which stands for eXtreme Gradient Boosting, offers a highly efficient way to implement the gradient boosting framework that works well for both linear and tree-based models, making it especially suited for large datasets. The fundamental concept underlying XGBoost involves training a series of basic models, including decision trees, and then combining their predictions to produce a more robust and accurate overall model. By using boosting to train each tree to correct the errors of the previous trees in the sequence, XGBoost can achieve even better results.\n",
        "\n",
        "XGBoost is powered by gradient boosting, a technique used to optimize the parameters of decision trees by minimizing the loss function. This approach involves adjusting the tree parameters to reduce the overall error of the model. Additionally, XGBoost comes with several other features, including regularization, which can help to prevent overfitting by constraining the complexity of the model, and parallel processing, which enables faster training times by splitting up the computations across multiple processors."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Explainablity**"
      ],
      "metadata": {
        "id": "FXnexIjDSEnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from XGBoost model\n",
        "importances = xgb_model.feature_importances_\n",
        "\n",
        "# Create a dictionary mapping feature names to importances\n",
        "feature_dict = dict(zip(df.columns, importances))\n",
        "\n",
        "# Sort the features by importance, in descending order\n",
        "sorted_features = sorted(feature_dict.items(), key=lambda x: x[1])\n",
        "\n",
        "# Extract the sorted feature names and importances as separate lists\n",
        "sorted_feature_names = [feat[0] for feat in sorted_features]\n",
        "sorted_importances = [feat[1] for feat in sorted_features]\n",
        "\n",
        "# Plot the feature importances as a horizontal bar chart\n",
        "plt.figure(figsize=(10,20))\n",
        "\n",
        "plt.title('Feature Importance', fontsize=25)\n",
        "plt.barh(sorted_feature_names, sorted_importances, color='red', align='center')\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.ylabel('Features', fontsize=20) "
      ],
      "metadata": {
        "id": "aHBgrE_RYIEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plot gives the average feature shapley values."
      ],
      "metadata": {
        "id": "HWc7rWXPG0Qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While XGBoost has been chosen as the optimal model due to its impressive accuracy, it is often referred to as a black box model, as it lacks transparency in revealing the inner workings of the algorithm. In order to increase stakeholder confidence and trust, it is crucial to provide meaningful explanations for the model's predictions, underlining the conditions that lead to the final outcomes. To enhance model explainability, we have generated a descending bar plot of feature importance, effectively shedding light on the most impactful features and their contribution towards the predictions."
      ],
      "metadata": {
        "id": "vRJ-e-pNa3eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While linear regression helped us achieve an accuracy of 85%, it became necessary to employ more advanced models like random forest, Xgboost, and decision tree to further enhance accuracy and grow our business. However, these models are considered black box models as they are difficult to explain. To overcome this limitation and improve model explainability, we leveraged the power of the SHAP (SHapley Additive exPlanations) model explainability tool"
      ],
      "metadata": {
        "id": "-rJ3geKOcEcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**SHAP** **(Shapley Additive exPlanations)**"
      ],
      "metadata": {
        "id": "Qboe4G6bcmMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "PBSTeq9Fc9lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import shap \n",
        "sns.set_style('darkgrid')"
      ],
      "metadata": {
        "id": "r8EAGU7OdffR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature = df.columns[:-1]"
      ],
      "metadata": {
        "id": "x_qNI4gcCrqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature"
      ],
      "metadata": {
        "id": "TRBMPdoCCKe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[0:1]"
      ],
      "metadata": {
        "id": "BTW82cNxDvFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the JavaScript visualizations in the notebook environment\n",
        "shap.initjs()\n",
        "\n",
        "# Create a TreeExplainer object for the best XGBoost model from the grid search\n",
        "explainer = shap.TreeExplainer(xgb_model)\n",
        "\n",
        "# Calculate SHAP values for the first row of the test data\n",
        "shap_values = explainer.shap_values(X_test[0:1])\n",
        "\n",
        "# Plot the SHAP force plot for the first row's explanation\n",
        "shap.force_plot(explainer.expected_value,features = feature, shap_values=shap_values[0])\n"
      ],
      "metadata": {
        "id": "qaLgDUF_gJxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They tell the contribution of each feature in increasing or decreasing the final prediction of the dependent variable.\n",
        "\n",
        "Note that these shapley values are valid for this observation only. With other data points the SHAP values will change."
      ],
      "metadata": {
        "id": "narNoa4MGllw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0:1]"
      ],
      "metadata": {
        "id": "cZ6lhdm3DcEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# second sample test\n",
        "shap_values = explainer.shap_values(X_test[1:2])   "
      ],
      "metadata": {
        "id": "lg1V8MjSF0zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values"
      ],
      "metadata": {
        "id": "pAXx6luBF4eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##begin the JavaScript visualisation in the notebook environment.\n",
        "shap.initjs() \n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)"
      ],
      "metadata": {
        "id": "thM4AU-PH4Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# third sample test\n",
        "shap_values = explainer.shap_values(X_test[2:3])"
      ],
      "metadata": {
        "id": "wppcZwJvIPus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#begin the JavaScript visualisation in the notebook environment.\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)\n",
        "     "
      ],
      "metadata": {
        "id": "_63fx_dQIUCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q-Yo_11gOl7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Conclusions drawn from EDA**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) is an initial and essential step in data analysis, where we explore and examine the dataset to gain insights, find patterns, and detect anomalies . it also helps us to identify relationships and correlations between the variables and get a better understanding of how they affect each other ,  It aids in seeing any patterns, irregularities, and correlations in the data as well as any potential problems such missing values or outliers.  it enables us to truly understand the data, laying the foundation for insightful decision-making that can lead to groundbreaking discoveries"
      ],
      "metadata": {
        "id": "S7CW5AGsSV6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we began by loading the dataset and then conducted an Exploratory Data Analysis (EDA) to examine all of its features. We then focused on our dependent variable, \"Rented Bike Count,\" which we transformed and treated for null values. After that, we performed feature selection and categorical column encoding. We also analyzed the numeric variables and identified highly correlated variables, which we subsequently dropped. Additionally, we conducted one-hot encoding and built the model. Finally, we extracted statistical information that proved to be useful for business purposes."
      ],
      "metadata": {
        "id": "qBhKoF5FTzD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "\n",
        "*  Given that the distribution of the dependent variable must be normal for linear regression to work, the dependent variable appears to be moderately right skewed in the distribution plot shown above. As a result, we have  performed some operations to make the distribution of the dependent variable normal.\n",
        "* Outliers can be found in the rented bike count data, as seen by the boxplot above. After the removal of outliers from the square root transformation we got normal distribution\n",
        "\n",
        "\n",
        "\n",
        "*  We obtain an almost normal distribution after applying the square root to the skewed Rented Bike Count. As a result, we may perform the square root transformation during modelling.\n",
        "\n",
        "* As we can see from the above displot that Normally distributed attributes: temperature , hour , humidity. Positively skewed attributes: wind, rented bike count , solar_radiation, snowfall, rainfall. Negatively skewed attributes: visibility.\n",
        "\n",
        "\n",
        "\n",
        "*  Hour, Temperature, Wind Speed, Visibility, and Solar Radiation are all positively correlated with the dependant variable. This implies that the number of rented bikes rises as these features do, while the columns \"Rainfall,\" \"Snowfall,\" and \"Humidity\" are those features that have a negative relationship with the dependent variable, suggesting that the number of rented bikes falls as these features rise.\n",
        "*  Our collection primarily includes data from the year 2018 and only a little amount from the year 2017.\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "*   We have found that average of the rented bike counts is higher during the summer and lowest during the winter.\n",
        "*  We can observe that the months of December, January, and February—the winter seasons—have lower demand for rented bikes than those months, as well as May, June, and July—the summer seasons—which have the highest demand for bikes.\n",
        "\n",
        "\n",
        "*  As we have observed that , all days, rented bike count is consistant and equal.\n",
        "*  I am able to see the valuable insights about the distribution of the 'holiday' column in my dataset. The chart revealed that the majority of the ratings - a whopping 95.1% or 8,328 records - were on non-holiday days. In contrast, the number of ratings received during holidays was relatively low, accounting for only 4.9% or 432 records of the total rented bike count data available in the dataset. These findings highlight the importance of considering external factors, such as holidays, when analyzing data, as they can have a significant impact on the trends and patterns observed in the dataset.\n",
        "\n",
        "\n",
        "*   People favour rented bikes during rush hour, as evidenced by the high surge in hired bikes from 8:00 am to 9:00 pm. We may state that during business opening and closing times there is a significantly high demand because it is apparent that demand increases most at 8 a.m. and 6:00 p.m.\n",
        "\n",
        "*  there we have observed that 96.6% of the dataset consists of non-functional days, while the remaining 3.4% represents weekends. This indicates that there is a significantly higher demand for bikes on functional days compared to weekends, where the demand is relatively low.\n",
        "\n",
        "*  We can observe a decline in demand for rented bikes when snow falls.\n",
        "*   it is clear that there is a sharp increase in the use of rented bikes between the hours of 8:00 a.m. and 9:00 p.m., suggesting that individuals prefer to use rented bikes during peak hours, perhaps for their commute to work. In addition, it can be seen that the demand for rental bikes is greater on non-holiday days than it is on days with holidays.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* By analyzing the Graph, it can be concluded that the average number of rented bikes remains relatively stable from Monday to Saturday. However, there is a noticeable dip in bike rentals on Sundays, and on average, the number of rented bikes is significantly lower on weekends than on weekdays.\n",
        "\n",
        "* The analysis of the graph reveals that the demand for rented bikes is lower during the winter months, specifically December, January, and February, in contrast to the summer months of May, June, and July, which exhibit the highest demand. Additionally, the graph indicates a significant surge in rented bike usage between 8:00 a.m. and 9:00 p.m., highlighting the preference of individuals to rent bikes during peak hours, likely for their daily commute to work.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   The temperature(°C) and dew point temperature(°C) columns of this graph demonstrate multicollinearity, as can be seen.\n",
        "*   The above graph showed less linear correlations between variables and non-linear separability of data points, which was new to me. Hence, we may conclude that both positive and negative trends are present in the relationship between each column in the graph.\n"
      ],
      "metadata": {
        "id": "zeOrFsrIkO8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Conclusions drawn from ML Model Implementation**"
      ],
      "metadata": {
        "id": "z_uj6BkqsvFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The success of any business heavily relies on the accuracy of its machine learning models. Therefore, it's crucial to thoroughly evaluate the model's performance and predictions before deploying it in the real world. This evaluation process helps to identify the model's strengths and weaknesses, ensuring that it's fully prepared and ready for deployment."
      ],
      "metadata": {
        "id": "_Hvko8sTtwTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll discuss some essential factors that apply to all ML models and then delve into the project-specific conclusions we've drawn. By doing so, we'll gain a better understanding of the model's overall performance and its impact on the business's growth."
      ],
      "metadata": {
        "id": "6Mke47lqt9sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explain the findings and insights we've gained from evaluating our ML model, which will help us make informed decisions on the next steps in the deployment process."
      ],
      "metadata": {
        "id": "DxyuYkX3uLuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model conclusiuon**"
      ],
      "metadata": {
        "id": "Zx1F5fW7ujJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have experimented with several regression models, beginning with Linear Regression and moving on to other non-linear models. With each model, we've carefully tuned the hyperparameters to minimize errors and ensure optimal performance."
      ],
      "metadata": {
        "id": "qaePTK8yumdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By systematically testing and evaluating these models, we've gained a deep understanding of their capabilities and limitations, providing us with valuable insights into how we can optimize our ML strategy to drive business growth."
      ],
      "metadata": {
        "id": "0RUvMG5LvNwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Even after employing regularisation techniques, the model in linear regression still captures 70% of variance and has a r2 of 84%, indicating that the target variable and our data are not entirely linearly dependent on one another.\n",
        "\n",
        "\n",
        "\n",
        "*  After experimenting with the Decision Tree model, we found that we achieved an impressive R-squared value of 86% accuracy with a maximum depth of 10. However, as we increased the depth beyond this point, we noticed that the model began to overfit the data. This led to a higher Mean Squared Error (MSE) of 0.361\n",
        "\n",
        "\n",
        "\n",
        "*  We discovered that prioritizing individual variables when building our model did not always lead to the best accuracy. Instead, using ensemble techniques like Random Forest and optimizing hyperparameters allowed us to create a more robust and accurate model. By doing so, we achieved an impressive R-squared value of 92% with 100 trees in the forest.\n",
        "*   while we achieved a slightly lower R-squared value of 87% using the GradientBoostingRegressor, we found that it offered the advantage of faster results due to its ability to utilize all available cores. This increased efficiency led to a reduction in processing time, making it a valuable tool for projects that require faster results.\n",
        "\n",
        "\n",
        "\n",
        "* After extensive experimentation, we ultimately settled on XGBoost as our final model. This decision was driven by the impressive results we achieved, including an R-squared value of 97% and a mean squared error (MSE) of 0.177. These results suggest that our model is highly accurate and reliable, providing us with a valuable tool for predicting outcomes and driving business growth.\n",
        "\n"
      ],
      "metadata": {
        "id": "LumnM39-yDI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}